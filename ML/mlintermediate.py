# -*- coding: utf-8 -*-
"""MLintermediate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vZkF59TrU2L6iHfUQMU8hHdFpHbLKUhV
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras import models,losses,layers
import re
import os

from google.colab import drive
drive.mount('/content/drive')

dir='/content/drive/My Drive/Colab Notebooks/Intermediate_Assignment_Dataset'
total_train=[]
total_validation=[]
total_test=[]
for filename in os.listdir(dir):
  if filename.startswith('No'):
    if int(filename[12:-4])<251:
      total_train.append([filename,0])
    else:
      total_test.append([filename,0])
  else:
    if int(filename[13:-4])<252:
      total_train.append([filename,1])
    else:
      total_test.append([filename,1])
import random
for ds in [total_train,total_test]:
  random.shuffle(ds)
total_test

img=os.path.join(dir+'/',total_train[4][0]) #data exploration
with open(img) as im:
  im=im.read().splitlines()
im=im[3:]
for line in im:
  l=line.split()
  for i in l:
    n=int(i)
    l[l.index(i)]=n
  im[im.index(line)]=l
img=np.array(im)/255.0
img.shape

for ds in [total_train,total_test]:
  for d in ds:
    img =os.path.join(dir+'/',d[0])
    im=None
    with open(img) as im:
      im=im.read().splitlines()
      im=im[3:]
      for line in im:
        l=line.split()
        for i in l:
          n=int(i)
          l[l.index(i)]=n
        im[im.index(line)]=l
    img=np.array(im,dtype=np.float32)/255.0
    img=img.reshape((960,16,1))
    ds[ds.index(d)][0]=img
print(total_test[1][0].shape)

model=models.Sequential([layers.Conv2D(32,(3,3),padding='same',input_shape=(960,16,1),activation='relu'),layers.MaxPooling2D(padding='same'),
                         layers.Conv2D(64,(3,3),activation='relu'),layers.MaxPooling2D(padding='same'),
                         layers.Conv2D(128,(3,3),activation='relu'),layers.MaxPooling2D(padding='same'),
                         layers.Dropout(0.2),layers.Flatten(),
                         layers.Dense(512,activation='relu'),
                         layers.Dense(1)
                         ])
model.compile(optimizer='adam',loss=losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'])
model.summary()

epochs=10
batch_size=50
x=[]
y=[]
for i in range(len(total_train)):
  x.append(total_train[i][0])
  y.append(total_train[i][1])
x=np.array(x)
y=np.array(y)
model.fit(x,y,epochs=epochs,validation_split=0.4,batch_size=batch_size,verbose=2,steps_per_epoch=int(np.ceil(epochs/batch_size)))

x=[]
y=[]
for i in range(len(total_test)):
  x.append(total_train[i][0])
  y.append(total_train[i][1])
x=np.array(x)
y=np.array(y)
model.evaluate(x=x,y=y,batch_size=batch_size)